{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2159eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "# Add any other imports you need here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d71de2ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms' has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mTransform, resize and normalize the images and then use a pretrained model to extract \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mthe embeddings.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO: define a transform to pre-process the images\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# The required pre-processing depends on the pre-trained model you choose \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# below. \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# See https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_transforms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m ])\n\u001b[1;32m     17\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtrain_transforms)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#input_batch = train_dataset.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# run out of memory (VRAM if on GPU, RAM if on CPU)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.transforms' has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "the embeddings.\n",
    "\"\"\"\n",
    "# TODO: define a transform to pre-process the images\n",
    "# The required pre-processing depends on the pre-trained model you choose \n",
    "# below. \n",
    "# See https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n",
    "#input_batch = train_dataset.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "# run out of memory (VRAM if on GPU, RAM if on CPU)\n",
    "batchsize=4\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batchsize,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True, num_workers=4)\n",
    "\n",
    "# TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n",
    "# more info here: https://pytorch.org/vision/stable/models.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7482266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 10000\n",
      "    Root location: dataset/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdc85b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "data_without_label=[]\n",
    "for i in np.arange(10):\n",
    "    data_without_label.append(train_dataset[i][0])\n",
    "data_without_label\n",
    "model=models.resnet18(pretrained=True)\n",
    "#t=train_dataset[0][0].unsqueeze(0)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "#embedding_size = list(model. children())[-3][-1][-2].num_features\n",
    "#print(summary(list(model.children())[-1]))\n",
    "#embedding_size = list(model. children())[-1].size\n",
    "#model(t).shape\n",
    "x=train_dataset[0][0].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    embedding_size = model(x).squeeze().numpy().shape[0]\n",
    "    print(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f71f0486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1000)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "      model.eval()\n",
    "      \n",
    "      for batch_index, (X,_) in enumerate(train_loader,0):\n",
    "        \n",
    "        yp = model(X)\n",
    "        yp = yp.squeeze().cpu().numpy()\n",
    "        if batch_index==1:\n",
    "            break\n",
    "        print(yp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cc3f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fc3f2d6fcd0>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c195abae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 5, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 5x7\n",
    "m = nn.AdaptiveAvgPool2d((5, 7))\n",
    "input = torch.randn(1, 64, 8, 9)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "# target output size of 7x7 (square)\n",
    "m = nn.AdaptiveAvgPool2d(7)\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "# target output size of 10x7\n",
    "m = nn.AdaptiveAvgPool2d((None, 7))\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "013fa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00022\n"
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "with open('train_triplets.txt') as f:\n",
    "    for line in f:\n",
    "        triplets.append(line)\n",
    "train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                         transform=None)\n",
    "filenames = [s[0].split('\\\\')[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "print(filenames[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24ede46",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of ints, but found element of type list at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     layer1\u001b[38;5;241m=\u001b[39m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     y\u001b[38;5;241m=\u001b[39mlayer1(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of ints, but found element of type list at pos 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "x=torch.tensor([[1,0],[0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1760dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
