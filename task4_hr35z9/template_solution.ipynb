{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57002395eb6b50f8",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "This serves as a template which will guide you through the implementation of this task. It is advised to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps.\n",
    "This is the jupyter notebook version of the template. For the python file version, please refer to the file `template_solution.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46aca0e5d9ef",
   "metadata": {},
   "source": [
    "First, we import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c6ff7632991155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, get_scheduler, AutoTokenizer, DistilBertModel\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model_transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "#nlp = pipeline(\"sentiment-analysis\", model=\"model/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# Add any other imports you need here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cba49",
   "metadata": {},
   "source": [
    "Depending on your approach, you might need to adapt the structure of this template or parts not marked by TODOs.\n",
    "It is not necessary to completely follow this template. Feel free to add more code and delete any parts that are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9b5e89d4b3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([54, 768])\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "BATCH_SIZE =4  # TODO: Set the batch size according to both training performance and available memory\n",
    "NUM_EPOCHS = 15  # TODO: Set the number of epochs\n",
    "\n",
    "train_val = pd.read_csv(\"train.csv\")\n",
    "test_val = pd.read_csv(\"test_no_score.csv\")\n",
    "#len(test_val)\n",
    "#test_val[\"title\"]+\". \"+test_val[\"sentence\"]\n",
    "#test_val[\"title\"][0]\n",
    "index=55\n",
    "cat=train_val.iloc[index][\"title\"]+\". \"+train_val.iloc[index][\"sentence\"]\n",
    "inputs = tokenizer(cat, return_tensors=\"pt\")\n",
    "#result=nlp(cat)\n",
    "#print(result)\n",
    "outputembedding = model_transformer(**inputs)\n",
    "print(outputembedding.last_hidden_state.squeeze().size())\n",
    "#print(outputembedding)\n",
    "model_transformer=model_transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "161fdafaedaa5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill out ReviewDataset \n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, data_frame,train=True):\n",
    "        self.data=data_frame\n",
    "        self.train=train\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            label=self.data[\"score\"][index]\n",
    "            cat=self.data.iloc[index][\"title\"]+\". \"+self.data.iloc[index][\"sentence\"]\n",
    "            inputs = tokenizer(cat, return_tensors=\"pt\")\n",
    "            output = model_transformer(**inputs)\n",
    "            embeddings=output.last_hidden_state.squeeze()[1]\n",
    "            #result = nlp(cat)[0]\n",
    "            return embeddings,label\n",
    "        \n",
    "        else:\n",
    "            cat=self.data.iloc[index][\"title\"]+\". \"+self.data.iloc[index][\"sentence\"]\n",
    "            inputs = tokenizer(cat, return_tensors=\"pt\")\n",
    "            output = model_transformer(**inputs)\n",
    "            embeddings=output.last_hidden_state.squeeze()[1]\n",
    "            return embeddings\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "676f7a012f50e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ReviewDataset(train_val)\n",
    "test_dataset = ReviewDataset(test_val,train=False )\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "# Additional code if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dd6eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-2.1767e-01,  1.9591e-01,  3.3385e-01,  1.2837e-01,  4.9409e-01,\n",
      "         3.5498e-01, -1.8685e-01,  3.4219e-01,  3.9462e-02, -4.4583e-01,\n",
      "        -1.0806e-01, -2.3602e-02,  1.2929e-02, -9.7192e-02, -3.7490e-01,\n",
      "        -1.2347e-01,  4.6782e-01, -1.5139e-01, -1.7128e-01, -1.9161e-01,\n",
      "         5.8029e-01, -1.7494e-01,  1.8317e-02,  3.1652e-01, -1.1008e-01,\n",
      "         3.8947e-01,  3.1195e-01,  2.1515e-01, -2.5271e-01, -2.1922e-01,\n",
      "         6.4876e-01, -2.7088e-02, -8.3678e-02, -2.0793e-01, -6.3247e-01,\n",
      "        -7.7573e-01,  1.2777e-01,  1.4343e-01, -1.0237e-01,  5.4094e-01,\n",
      "         7.4926e-02, -3.1621e-01,  4.1345e-01,  3.9535e-01, -1.3617e-01,\n",
      "        -1.2860e-01,  8.1023e-03,  1.6361e-01,  2.0293e-02, -7.5038e-01,\n",
      "        -5.1290e-02,  2.8841e-01,  4.8107e-02,  2.9240e-01, -1.0673e-01,\n",
      "         5.6185e-01, -4.5827e-01,  3.3699e-01, -2.2345e-01, -2.9871e-01,\n",
      "         1.3222e-01, -7.1173e-02,  1.6139e-01, -2.7798e-01,  2.5079e-01,\n",
      "         1.6671e-01,  2.5110e-02, -3.2889e-01, -4.2552e-01,  2.5443e-01,\n",
      "        -1.6990e-01, -3.1177e-01, -7.7027e-02, -1.6685e-01, -1.3777e-02,\n",
      "        -4.7918e-01,  3.2241e-02, -2.6889e-01,  1.5443e-01, -1.6001e-01,\n",
      "         4.8744e-01, -7.0676e-01, -4.6295e-01,  3.0501e-01, -2.5862e-01,\n",
      "         1.5494e-01, -1.1845e-02, -1.0144e-01, -1.0197e-01,  6.0069e-01,\n",
      "        -1.3899e-03, -2.7344e-01,  2.5500e-01,  1.7400e-01,  1.3130e-01,\n",
      "        -3.4035e-01, -7.6768e-03, -2.0459e-01,  1.3270e-01,  3.3441e-01,\n",
      "         4.0100e-01, -1.7667e-01,  1.4821e-01, -3.2768e-01, -4.0549e-01,\n",
      "        -2.4156e-01, -6.7258e-02,  3.7808e-01, -3.1394e-01, -2.3172e-01,\n",
      "         3.0782e-02,  1.1819e-01,  3.4646e-02, -7.1227e-02, -2.5926e-01,\n",
      "         2.3750e-01, -4.9544e-01,  1.9121e-01,  3.7097e-01,  3.4213e-01,\n",
      "         4.2953e-01,  5.8299e-01,  4.4961e-02,  2.5799e-01,  1.5069e-01,\n",
      "         5.4418e-02, -2.0750e-01,  5.3122e-01,  8.6824e-02, -2.4090e-01,\n",
      "         2.4676e-01, -2.1017e-02,  3.8212e-02,  3.1638e-01, -2.8388e-01,\n",
      "         1.1452e-01, -3.9408e-02,  2.4848e-02, -2.6591e-01, -1.3510e-01,\n",
      "         3.9514e-01,  4.5797e-01, -7.8648e-02,  2.7619e-01,  3.2822e-01,\n",
      "        -1.6840e-01, -3.4309e-01, -2.8570e-01, -8.6663e-02,  2.3926e-01,\n",
      "        -3.4357e-03,  2.2954e-01, -2.0918e-02, -2.6914e-02,  1.5907e-01,\n",
      "         4.1112e-01, -3.6835e-01,  1.2785e-01,  8.2317e-02,  5.1690e-03,\n",
      "        -1.5191e-01,  7.3119e-02,  1.2024e-01, -1.4222e-01, -5.2539e-01,\n",
      "        -1.2864e-01, -5.3692e-01,  2.4564e-01, -5.8031e-01, -1.4927e-01,\n",
      "         3.2009e-01, -1.1311e-01,  5.9953e-01,  4.2012e-02,  1.9573e-01,\n",
      "        -2.4021e-01,  7.9950e-01,  1.1259e-01,  3.2460e-01, -1.3669e-01,\n",
      "        -3.3095e-01,  7.4409e-01, -2.2278e-02, -2.6851e-01,  1.9430e-01,\n",
      "         2.0139e-01,  3.4735e-01, -3.4009e-01,  3.9873e-01,  4.6865e-01,\n",
      "         3.9297e-01, -2.1821e-01, -1.9861e-01, -2.8168e-01,  4.6104e-01,\n",
      "         3.8804e-01,  6.0739e-01,  5.4396e-02,  1.3672e-01, -3.5435e-01,\n",
      "        -3.8671e-01, -5.4891e-02,  2.9883e-01,  3.2458e-01, -2.9546e-01,\n",
      "         4.3420e-01,  3.3119e-02,  1.2105e-01, -1.4494e-01,  5.4993e-01,\n",
      "         2.9269e-01,  3.5437e-01,  5.5314e-01, -9.9910e-02,  4.1283e-02,\n",
      "         2.1347e-01, -4.1869e-03,  5.2148e-01,  4.5903e-01, -8.2974e-02,\n",
      "         4.2477e-01,  1.9562e-01, -3.0811e-01,  6.6331e-01, -1.6195e-01,\n",
      "         6.5148e-01,  4.2089e-01, -4.2808e-01,  1.6570e-01,  1.1176e-02,\n",
      "        -1.6807e-01, -3.9651e-01,  1.4274e-02,  2.9840e-01,  1.8974e-01,\n",
      "        -2.5568e-01, -4.9436e-01,  1.8232e-01, -1.7784e-01,  3.4581e-01,\n",
      "         2.7810e-02, -1.5119e-02,  1.8720e-01,  1.0491e-01, -3.4105e-01,\n",
      "         1.1239e-01, -1.9134e-01, -2.7778e-01, -5.8881e-01, -1.3744e-01,\n",
      "        -1.5227e-01, -5.5664e-01,  3.6170e-02, -2.6783e-01,  5.7725e-01,\n",
      "        -3.4344e-02, -2.5252e-01,  1.5657e-01, -5.0267e-02, -3.3741e-03,\n",
      "         3.7637e-01, -9.8244e-02, -3.2646e-01, -4.7907e-01, -3.3225e-01,\n",
      "        -4.7569e-01, -2.6588e-01,  6.0466e-01,  1.8919e-01,  1.3137e-01,\n",
      "         2.9430e-01,  3.8444e-01, -1.6988e-01, -2.4500e-01, -5.5637e-01,\n",
      "        -4.2287e-01,  7.1841e-01,  4.2962e-01, -1.7456e-01, -1.3905e-01,\n",
      "        -1.5915e-01,  1.1011e+00,  9.9949e-02,  2.1713e-01,  5.0843e-02,\n",
      "        -1.0104e-01,  4.4938e-01, -7.2935e-01, -7.3494e-01, -6.2850e-01,\n",
      "        -2.3277e-01, -1.4296e-02, -5.7427e-01, -3.9223e-01,  3.7967e-01,\n",
      "         8.1744e-02,  2.6071e-01, -2.8105e-01, -2.9957e-01, -4.4258e-01,\n",
      "         4.1675e-01, -6.2108e-02, -8.7561e-02, -7.3502e-01,  2.6908e-01,\n",
      "         1.9674e-01, -2.1368e-01, -3.4663e-02, -4.7491e+00, -1.1672e-01,\n",
      "        -3.2352e-01,  3.0902e-01,  2.0297e-01, -4.9420e-01, -7.7474e-03,\n",
      "        -1.7456e-01, -1.3690e-01, -3.5764e-02,  7.5026e-02, -5.8485e-02,\n",
      "        -9.2681e-03,  3.9686e-01,  3.8170e-01,  3.2994e-01, -6.5579e-01,\n",
      "         4.9394e-01, -1.5146e-01,  4.4690e-01, -5.6081e-02, -1.5031e-01,\n",
      "         1.0093e-01, -1.7719e-01, -6.6241e-02,  5.5170e-01, -3.3244e-01,\n",
      "        -1.6396e-01, -3.1447e-01,  5.0567e-01, -2.1659e-01,  2.1754e-01,\n",
      "         2.3840e-01,  4.3997e-01,  3.0611e-01, -1.5100e-01, -8.3485e-02,\n",
      "        -6.1214e-02, -2.5014e-01,  1.9430e-01, -4.3779e-02, -8.9344e-02,\n",
      "        -6.2284e-01,  5.6430e-01,  3.2573e-01,  1.5767e-01,  7.6927e-02,\n",
      "        -9.9578e-02,  1.4657e-01,  6.0331e-02,  6.7284e-02, -3.1546e-01,\n",
      "        -3.2388e-03,  5.2258e-01, -1.1626e-01, -1.3450e-01,  2.3579e-01,\n",
      "         7.9915e-01,  1.0828e-01, -1.6135e-01, -3.9784e-01, -2.3401e-01,\n",
      "        -1.1470e-01,  1.8332e-01, -2.9188e-01,  3.4464e-01,  3.5616e-02,\n",
      "        -4.5596e-01, -7.8503e-02, -9.1382e-02, -1.9259e-02, -1.0572e-01,\n",
      "         2.7176e-01, -4.7976e-01,  1.8471e-01, -7.9816e-02, -4.8079e-01,\n",
      "         1.7985e-02, -1.7986e-02,  6.7552e-01,  4.6166e-01, -8.6525e-01,\n",
      "         1.9818e-01,  2.6694e-01,  1.5839e-01, -5.6843e-01,  3.4300e-01,\n",
      "        -4.1564e-01, -4.1301e-01,  2.2218e-01,  8.6092e-02, -2.1147e-01,\n",
      "        -8.3740e-02, -2.5857e-02,  1.1249e-02,  1.4289e-01,  3.1382e-01,\n",
      "        -8.0742e-01,  2.3673e-01, -2.9814e-01, -2.4775e-01,  2.8135e-02,\n",
      "         5.5708e-01, -5.4353e-01,  1.1746e-01, -1.6699e-01,  7.9213e-02,\n",
      "         2.7365e-01, -4.6588e-02, -2.4006e-01,  4.1208e-01, -4.8577e-01,\n",
      "        -4.6525e-02,  7.8672e-03, -1.5824e-01, -1.2958e-01, -3.5319e-01,\n",
      "        -1.4297e-01, -4.0700e-01, -2.7576e-03,  2.7107e-01,  4.9831e-01,\n",
      "        -2.1363e-01, -3.8330e-01, -7.5430e-01,  2.5354e-01,  7.3372e-02,\n",
      "        -1.7173e-01, -3.2999e-01,  5.8111e-02, -2.2298e-01,  5.5580e-02,\n",
      "        -1.5526e-01,  1.1245e-01,  4.3720e-02, -5.7698e-03, -3.3781e-01,\n",
      "        -2.9935e-01,  2.5419e-01,  5.2688e-02,  3.5975e-01,  8.1695e-02,\n",
      "         4.9376e-02,  1.9205e-01,  5.8697e-02,  3.8854e-01,  4.6254e-01,\n",
      "        -1.8954e-01, -2.5127e-02, -5.5993e-02,  1.0524e-01, -2.5362e-01,\n",
      "        -4.0950e-01,  9.6224e-02, -7.3448e-01, -9.1628e-02,  5.0330e-01,\n",
      "         1.7848e-01, -3.7946e-01, -4.3289e-02, -7.3345e-02, -3.9294e-02,\n",
      "        -7.7198e-01,  3.8539e-01, -3.2306e-01,  4.2409e-01, -7.8428e-02,\n",
      "        -1.4832e-01, -1.1201e-01,  1.1317e-01,  5.8686e-01,  6.5967e-01,\n",
      "        -2.8761e-01, -1.0683e-01, -1.0678e-01,  1.8248e-01, -2.2302e-01,\n",
      "        -3.4237e-01, -2.4770e-01,  1.3908e-01,  6.8683e-02, -3.5312e-01,\n",
      "         7.9604e-02, -1.6651e-01, -5.3145e-01,  1.6947e-01,  9.4428e-02,\n",
      "         2.8137e-02, -1.0695e-01,  3.9741e-02,  2.7523e-01, -2.7273e-01,\n",
      "         1.9501e-01,  9.0172e-02, -2.2334e-01, -2.0756e-01,  5.7058e-02,\n",
      "        -2.6527e-01, -4.8607e-02, -2.5845e-01, -4.9563e-02,  3.0216e-01,\n",
      "        -5.8813e-01, -1.2330e-01, -1.8790e-01, -6.6481e-01,  1.2337e+00,\n",
      "         3.4932e-01, -3.4091e-02,  1.4341e-01, -1.1497e+00, -2.3792e-01,\n",
      "        -1.5399e-01, -1.0517e-01, -2.8199e-01, -3.6463e-01,  2.0307e-01,\n",
      "        -3.1317e-01, -8.2893e-02, -2.8395e-01,  8.7393e-02, -3.0794e-01,\n",
      "        -4.2462e-01, -4.3936e-02, -1.7879e-01, -1.7789e-01,  2.9752e-01,\n",
      "        -1.0057e-01,  1.5306e-01, -1.5923e-01,  1.1221e-01, -1.0573e+00,\n",
      "         2.2001e-01,  2.4215e-01,  2.2856e-01,  3.9801e-01, -7.3543e-01,\n",
      "         1.0303e-02,  1.6212e-01, -2.2108e-01, -6.6421e-02,  1.7769e-01,\n",
      "        -2.0303e-02, -2.8049e-01,  1.0220e-01,  4.0115e-01, -4.2508e-01,\n",
      "         4.4554e-01, -2.0418e-01,  2.4667e-01, -2.8872e-01, -1.4823e-01,\n",
      "         1.9528e-01, -3.1760e-01,  6.7383e-02, -3.4720e-01, -4.1662e-01,\n",
      "         3.1961e-02, -2.2339e-01,  1.8367e-01,  1.1028e-01, -1.3028e-01,\n",
      "         5.3977e-02,  2.9090e-01,  6.8305e-02,  3.5614e-01,  1.1548e-01,\n",
      "        -4.3151e-01, -2.8540e-01, -3.1443e-01,  2.1397e-01, -1.5189e-01,\n",
      "         2.4079e-01,  1.5845e-01,  4.2045e-01, -4.2490e-02, -4.7251e-01,\n",
      "        -6.7669e-03,  1.3137e-01,  1.2896e-01,  1.9947e-01,  6.2556e-01,\n",
      "        -5.3444e-01, -1.2030e-01,  2.5779e-01, -3.9609e-01,  2.0162e-02,\n",
      "         1.1607e-01, -2.1341e-01,  4.2208e-01,  3.6650e-01,  4.4986e-01,\n",
      "        -1.7589e-01, -5.1363e-01, -4.9300e-01, -4.8256e-01, -1.5549e-01,\n",
      "         3.7207e-01,  3.7938e-01,  1.6889e-01,  6.0002e-02,  3.2822e-01,\n",
      "        -8.7114e-03,  2.5032e-01, -4.6636e-01,  1.6232e-02,  2.5551e-01,\n",
      "        -4.2942e-01,  1.7733e-01,  9.1628e-02, -2.7009e-01, -4.9715e-01,\n",
      "         3.6893e-01,  3.7206e-01,  6.2631e-01,  6.3649e-02,  7.6957e-02,\n",
      "         1.6962e-01,  1.0166e-01, -2.6226e-01,  2.8463e-01,  8.7461e-02,\n",
      "         1.0003e-02, -1.0103e-01,  2.8137e-01,  5.5994e-01, -6.9528e-01,\n",
      "        -1.8058e-01,  8.1725e-01,  2.5065e-01,  3.9676e-01,  5.4388e-01,\n",
      "        -8.4655e-02,  1.9359e-01,  8.2807e-02, -1.1706e-01, -2.4233e-01,\n",
      "        -1.9750e-01,  1.9511e-02,  1.8526e-01, -8.1661e-02,  1.9457e-01,\n",
      "         7.5603e-01,  5.5103e-01,  5.8393e-01,  2.5538e-01, -3.1750e-02,\n",
      "        -3.3791e-01,  4.2256e-01,  2.5390e-01, -6.9385e-02,  2.4502e-01,\n",
      "        -4.9845e-02, -8.7034e-02, -1.4689e-01, -3.1912e-01, -9.3461e-02,\n",
      "         3.5178e-01, -3.9414e-01, -2.8466e-01,  1.1059e-01,  6.5839e-02,\n",
      "         6.4007e-02, -8.3376e-02,  2.1715e-01,  1.2401e-01, -2.9708e-02,\n",
      "         2.5163e-01, -4.5390e-01,  2.5038e-01, -2.7070e-01,  1.2202e-01,\n",
      "        -1.7597e-01,  2.2773e-01, -1.8620e-01,  2.4464e-01,  4.4110e-01,\n",
      "        -2.7166e-01,  5.5384e-02,  5.2570e-01, -1.9130e-01,  1.0379e-01,\n",
      "        -3.3548e-01, -2.5290e-01, -3.5584e-01, -5.8412e-01, -2.1044e-01,\n",
      "         1.6848e-01,  1.8896e-02, -4.2320e-01,  2.8511e-01, -1.1950e-01,\n",
      "         2.2753e-02, -3.5330e-01, -4.7535e-02, -4.4127e-01,  2.9957e-01,\n",
      "        -2.0993e-02, -1.9399e-02, -7.0924e-02,  6.3449e-01,  2.9616e-01,\n",
      "         9.4805e-03,  9.6671e-02, -5.6195e-02,  4.1052e-01, -2.8468e-01,\n",
      "         2.2132e-01, -7.0248e-01, -2.0756e-02,  3.6457e-01,  2.2721e-01,\n",
      "        -2.7413e-01,  5.3301e-02, -3.3603e-01,  6.5050e-01,  1.2596e-02,\n",
      "        -1.4750e-01, -2.2537e-01, -3.2486e-01,  1.2913e-01,  1.5105e-01,\n",
      "        -8.9643e-02,  3.0799e-01,  4.8324e-01, -6.4774e-01, -1.7634e-01,\n",
      "         8.6389e-02, -5.5628e-01, -4.6248e-01, -3.3411e-01, -4.7084e-02,\n",
      "         1.5570e-01,  1.4826e-02,  1.4565e-01,  8.1403e-02,  2.4738e-01,\n",
      "         1.5448e-01,  3.9857e-02, -2.2815e-01, -1.6930e-02,  1.1846e-01,\n",
      "         3.8371e-01, -7.9352e-02, -6.7594e-01,  2.5588e-01, -7.0755e-01,\n",
      "        -3.0131e-01, -7.6271e-02,  3.5414e-01, -2.2146e-01, -4.6190e-01,\n",
      "         8.4191e-03, -6.5329e-02,  5.1938e-03,  1.0243e-01, -2.8508e-01,\n",
      "         1.1671e-01,  2.7722e-01, -4.4828e-01], grad_fn=<SelectBackward0>), 7.531274096975738)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "843f38b9dbea00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill out MyModule\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(768, 2048)\n",
    "        self.l2=nn.Linear(2048,1)\n",
    "        #self.l3=nn.Linear(73,1)\n",
    "    def forward(self, x):\n",
    "        x=self.l1(x)\n",
    "        x=10*F.softmax(x)\n",
    "        x=self.l2(x)\n",
    "        #x=nn.ReLU(x)\n",
    "        #x=nn.Flatten(x)\n",
    "        #x=self.l3(x)\n",
    "        #x=10*F.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyModule().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e5bd0373a1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup loss function, optimiser, and scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.MSELoss()\n",
    "#optim = torch.optim.SGD(model.parameters(), lr=0.001,momentum=0.9)\n",
    "scheduler = None\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Current epoche:{epoch}')\n",
    "    run_loss = 0\n",
    "    model.train()\n",
    "    for [X,y] in tqdm(train_loader,total=len(train_loader)):\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        X=X.to(DEVICE)\n",
    "        y=y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        #loss = outputs.loss\n",
    "        loss=criterion(outputs,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        run_loss = run_loss + loss.item()\n",
    "        if i % 200 == 199:    # print every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {run_loss / 1000:.3f}')\n",
    "            run_loss=0\n",
    "        # TODO: Set up training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e56bba5fa2d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    results = []\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        batch = batch.to(DEVICE)\n",
    "        predicted = model(batch)\n",
    "        # TODO: Set up evaluation loop\n",
    "        results.append(predicted)\n",
    "    predictions = np.vstack(results)\n",
    "np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "print(\"Results saved to results.txt\")\n",
    "    #with open(\"result.txt\", \"w\") as f:\n",
    "        #for val in np.concatenate(results):\n",
    "            #f.write(f\"{val}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
